"""Gemini LLM wrapper."""
import asyncio
import traceback
import google.generativeai as genai
import config


class GeminiLLM:
    """Wrapper for Google Gemini LLM."""
    
    def __init__(self):
        """Initialize Gemini model."""
        print("ðŸ¤– Configuring Gemini LLM...")
        genai.configure(api_key=config.GEMINI_API_KEY)
        self.model = genai.GenerativeModel(config.GEMINI_MODEL)
        print("âœ… Gemini LLM configured")
    
    async def generate_response(self, user_text: str) -> str:
        """
        Generate response from user input.
        
        Args:
            user_text: User's input text
            
        Returns:
            Generated response text
        """
        try:
            prompt = f"{config.GEMINI_SYSTEM_PROMPT}\n\nØ§Ù„Ø¹Ù…ÙŠÙ„ Ù‚Ø§Ù„: {user_text}\n\nØ±Ø¯ Ø§Ù„Ù…ÙˆØ¸Ù:"
            
            # Run blocking API call in thread pool
            response = await asyncio.to_thread(
                self.model.generate_content, 
                prompt
            )
            
            return response.text.strip()
            
        except Exception as e:
            print(f"âŒ Gemini Error: {e}")
            traceback.print_exc()
            return "Ø¹Ø°Ø±Ù‹Ø§ ÙŠØ§ ÙÙ†Ø¯Ù…ØŒ Ø­ØµÙ„ Ø®Ø·Ø£ Ø¨Ø³ÙŠØ· ÙÙŠ Ø§Ù„Ù†Ø¸Ø§Ù…. Ù…Ù…ÙƒÙ† ØªØ¹ÙŠØ¯ Ø³Ø¤Ø§Ù„ÙƒØŸ"


# Global LLM instance
_llm_instance = None


def get_llm_model() -> GeminiLLM:
    """Get or create global LLM model instance."""
    global _llm_instance
    if _llm_instance is None:
        _llm_instance = GeminiLLM()
    return _llm_instance